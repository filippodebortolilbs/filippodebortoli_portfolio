---
title: "Formulating a Regression model for Salaries within a bank"
author: "Filippo de Bortoli & Jason Lubner"
date: "15/09/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



```{r,Load_libraries,echo = T, include = FALSE}
library(lmtest)
library(MASS)
library(car)
library(formula.tools)
library(leaps)
library(dplyr)
library(readr)
```

## Introduction

The data provided is from an anonymous bank, which includes the information on 208 employees of a certain bank, with 11 fields of interest filled with information of each employee. These are all middle ranking members of staff; excluding menial workers and senior management. Unfortunately, we are unable to specify the method of selection or if it includes all the middle ranking employees in the bank. However, we will assume it is simple random sampling, so it is unbiased though we cannot verify the precision of the data set.
In this report we are analysing the relation between the employee salary (in pounds) and the 10 other variables. The other variables include: Employee, Education level (“EducLev”); Job grade (“JobGrade”); Year born (“YrHired”); Gender (“Gender”); Performance review (“PerfRev”); IT job (“ITJob”); Years’ experience (“YrsExper”) and Age (“Age”).

We will be using all this information to produce a statistical regression model to predict the salary as well as analysing the final result in comparison to our hypothesis on which variables we would have thought to be the most influential on the salary of employees.




```{r,overview}
library(here)
bank <-read_csv("Bank.1.csv")
bank1=na.omit(bank) #removing data points with NA values
summary(bank1) # get a summary of the data
plot(bank1)#Overview of the data 
bank

```
## Methodology
Progression from Full Model to Final Model

## 1)	Initial Analysis of data
We acknowledged that the year born, and age were perfectly correlated variables and hence yielded equivalent values. From there, we began our analysis of the raw data in the bankl.xls file. We looked through the data for any possible “red flags” that could potentially affect the outcome. Employee (91), who had their education level as N/A was deemed as unreliable datum since there is no education level included within the range and we cannot assume or estimate said level. 

We also believed that the education level can have quite an impact on the salary of an employee thus, we removed it (using na.omit using R) and used the remaining 207 employee as our data set, which we believed would be substantial. 
There were a few other potential “red flags” particularly employee 208 who at 17-year-old is earning £59,000 which we deemed rather questionable due to his lack of experience in the banking sector, rather low Education level and job grade. We debated whether to remove this employee from the study. However, due to the number of employees being analysed this may have unnecessarily reduced the precision/accuracy of the regression model we created without further analysis to prove if it was indeed an outlier (which in fact proved not to be after checking outlier criteria).

```{r,estimate_models}
formula.fullmodel <- Salary ~ EducLev + JobGrade + YrHired + Gender + PerfRev + ITJob + YrsExper + Age
model.allvars = lm(formula.fullmodel,data=bank1)

summary(model.allvars)
plot(model.allvars)#RESIDUAL PLOT ANALYSIS
bptest(model.allvars)

hist(rstandard(model.allvars))

```
##2)	Regression Analysis
After, removing the questionable data with enough reasoning, we were ready to run our first regression but before that we looked at the plot (“plot” in pairs) of the all the variables. This proved our initial assumption that the year born, and age are the same, as they are perfectly correlated.

After this, we then ran the first regression using salary against all the remaining 8 other variables (“formula.fullmodel”). The F-test, is within the 5% significance level, which means there is a regression between salary and the variables. Looking at  the R squared value, which is quite high (close to 1) at 0.7891 shows the model is quite effective and that the data is close to the fitted regression line, as well as the adjusted R squared, 0.7806, being very close to the R squared, improving the reliability of the model. By analysing the co-efficients and their respective p-values  we can already see which variables show significance like JobGrade, Gender and YrsExper which are significant at 0.001 as well as YrHired, PerfRev significant at the 0.01 and 0.05 level respectively.

We then analysed the residual plots, which overall seemed to show correlation however, it could be improved. Firstly, the Q-Q plot, at both ends sways off the line of correlation so it doesn’t maintain the 45-degree line we would hope for, proving it isn’t perfectly distributed. The other graph that really stood out was residuals vs fitted since as it shows a “U-shaped” slope as well as heteroscedasticity issues. We were able to confirm our suspicions of heteroscedasticity by performing a studentized Breusch-Pagan test (“bp-test”); which concluded that the p value was significant, hence heteroscedasticity was present and we required a transformation. The Residual vs Leverage graph also indicated leveraging issues that needed adjustment.Finally reviewing a histogram of the residuals distribution seems to reflect a underlying normal distribution, however does reflect outliers at the tails.

```{r, transformations}

#formula.sqrt<- update.formula(formula.fullmodel,sqrt(Salary)~.)
#model.sqrt <- lm(formula.sqrt,data=bank1)
#bptest(model.log) #tried sqrt transformation

#formula.boxcox<- update.formula(formula.fullmodel,boxcox(Salary)~.,lamda=-0.5)
#model.sqrt <- lm(formula.boxcox,data=bank1)
#bptest(model.log) #tried sqrt transformation


formula.log <- update.formula(formula.fullmodel,log(Salary)~.)
model.log <- lm(formula.log,data=bank1)
plot(model.log)# the transformation seems have to reduced the U shaped issue and removed the heteroskedasticity 
bptest(model.log)# the test confirms the log transformation removes issues of heteroskedasticity



```
## 3)	Transformations
After realising the residual plots indicated heteroscedasticity we looked at performing a transformation. At first, we tried using the squared root of the salary to correct the heteroscedasticity test. However, when we checked it again using the bp-test it still indicated that it was heteroskedastic. Next, we applied a box-cox transformation. This gave a lambda of approx. -0.5; which got rid of the heteroscedasticity problem with a p value of 0.052. But we thought that this may not have cleared the issue once we removed outliers and thus  could further be improved, so we also conducted a logarithmic transformation on the salary (“formula.log”), just to see if we could find a more significant value. The bp-test on the log transformation yielded a p-value of 0.1344. This showed that the log transformation cleared the heteroscedasticity issues. We also analysed the graphs again which improved as slope was less curved and seemed to have more of a normal distribution (Q-Q plot). 
We then looked at the summary of the updated formula. The F-test is still significant, however r squared had decreased to 0.7844 as well as the adjusted R squared  falling to 0.7757. Though is proportion the percentage before and after the transformation is still roughly 1.1%. We also noticed that the significant variables had changed, so the only variable with significance at 0.001 was JobGrade, and the other significant variables then were, EducLev, Gender, PerfRev and YrsExper.
So, we decided to move forward with the log transformation due to it effectively being able to remove heteroscedasticity from the data set.

```{r,stepwise}
## Selecting dependent variables using stepwise method
step(model.allvars,direction = "both")
step(model.log,direction="both")
# reveals signficant variables were EducLev + JobGrade + YrHired + Gender + PerfRev + YrsExper
## Estimate models with selected variables
```


## 4)	Stepwise Analysis
At this point we were ready to run the stepwise analysis to determine which variables carried the most influence. We ran the stepwise analysis both ways (forwards and backwards) on the full model and the log transformed model. Both concluded by removing ITjob and Age. The only difference between both the full model and the log transformed model was the different coefficients for the remaining variables, which was expected.
We thus adapted our equation to match the stepwise formula under the independent variables of EducLev, Job Grade, YrHired, Gender, PerfRev and YearsExpr and assessed the results of the formula. 

```{r,outliers}
## Estimate models with selected variables
formula.step <- Salary ~ EducLev + JobGrade + YrHired + Gender + PerfRev + YrsExper
formula.log.step <- log(Salary) ~ EducLev + JobGrade + YrHired + Gender + PerfRev + YrsExper  
model.step <- lm(formula.step,data=bank1)
model.log.step <- lm(formula.log.step,data=bank1)

## Examine model and residuals
resids.step <- rstandard(model.step)
resids.log.step <- rstandard(model.log.step)
summary(model.step)
plot(model.step)
summary(resids.step) ## some large residuals here
summary(model.log.step)
plot(model.log.step)
summary(resids.log.step) ## some large residuals here
## Remove outliers and data with high leverage
summary(influence.measures(model.step))
summary(influence.measures(model.log.step))
outlierTest(model.step)
outliers.step <- c(1,5,10,14,66)# significant individuals from outlier test that need to be removed
outlierTest(model.log.step)
outliers.log.step <- c(1,5,10,14,139)# significant individuals revealed from log transformed that need to be removed from the log transform data, leverage significant level 3(p+1)/207=0.101
bank1.clean1 <- bank1[-outliers.step,]
bank1.log.clean1 <- bank1[-outliers.log.step,]

```
## 5)	Outliers
To test for outliers, we ran a summary on the influence of outliers to identify which employees were deemed as outliers in the data (“summary(influence.measure)”). Based on this we realised that the cooks distance showed no issues with outliers. However, the hat values on standardized residuals pointed out quite a few leverage points where above 0.101(  3(p+1)/n, for p variables and n individuals in the model as a standard for rejection) and thus would have to be removed. For both models the leverage points identified were 1,5,10 and 14.
We then performed the outlier test, to identify the outliers in the data. For The full model, the outlier identified was employee 66 and for the log transformed model  employee 139 was identified. This correlated with the previous residual graphs in Figures 1 and 2 used to analyse the full and transformed models; since they are points that are identified on the graph due to its spread-out locations on the graphs. So, we removed each from their respective models alongside the leverage points. Leaving both models, model.step.clean1 (full variable model) and model.log.step.clean1 (transformed model).

```{r,final models}
## New models estimated with cleaned data
model.step.clean1 <- lm(formula.step,data=bank1.clean1)
model.log.step.clean1 <- lm(formula.log.step,data=bank1.log.clean1)

plot(model.log.step.clean1)# new plots reveal less U SHAPED residuals thus the log transformation and outlier removal corrected issues.
## Test for normality and heteroskedasticity in residuals
bptest(model.step.clean1) ## residuals for the untransformed model fail the heteroskedasticity test
bptest(model.log.step.clean1) ## residuals for the log model pass the heteroskedasticity test

hist(rstandard(model.step))# the original model shows normality
hist(rstandard(model.log.step.clean1))#the new transformed model looks less normal so conduct a normality test to confirm still normal
shapiro.test(rstandard(model.log.step.clean1)) ## residuals for the log model pass the normality test p value greater than the 5% sig level

## Test for multicollinearity
vif(model.log.step.clean1) ## no VIF values > 10 
#cor(bank1.log.clean1[rhs.vars(formula.log.step)]) ## only one of the correlations is > 0.8 in magnitude bit only marginally so, vif shows not significantly enough
## Final model
summary(model.log.step.clean1)
```

## 6)	Final Regression and Checks
After improving our final model by removing the outliers according to the tests run previously, we then checked the residual and standardized residual plots to ensure the model (“model.log.step.clean1”)had corrected all issues identified above. Based on the graphs, we can see an improvement in our residual plots with a reduction in the U shaped curve as well an improved distribution based on the Q-Q plot

Leverage had been reduced from a 0.25 level to a 0.1 as shown in the Residuals vs leverage.
We checked for heteroscedasticity again, by repeating the bp-test for the transformed model, model.log.step.clean1; which showed that the model maintained its lack of heteroscedasticity. We checked normality by analysing the histograms for each model (Figure 5). The original data (“model.step.clean1”) showed normality. Our model, model.log.step.clean1, was much more skewed than the other histogram. 
To check that we still maintained normality, we ran the Shapiro Wilk normality test (“shapiro.test(rstandard())”), which gave a p value of 0.05481, which is above the 5% significance level and therefore still displayed normality.
Before we concluded we checked for multicollinearity by working out the variance inflation factors (“vif”), which would show multicollinearity issues if any of the variables have a value over 10. None of the variables displayed a value over 10, but the highest value was YrsExper at 4.2344. To check for any possible problems, we analysed the correlation (“cor”) between the variables. This showed a quite a high correlation between YrsExper and YrHired at -0.8, however its vif wasn’t high enough to be a problem, it would understandably be correlated and removing either of these variables considerably reduces the R-Squared value.
Therefore, our Final Model is the log of the response, with the YrBorn, Age and ITJob removed and all types of outliers removed too. The model at the end of step 5 (“model.log.step.clean1”) displayed at the beginning of this report which satisfies normality, has no heteroscedasticity  and  displays  adequate R-squared and adjusted R-squared values.

** Our final model is log(y) = -7.202197 + 0.024271x_1 + 0.10658x_2 + 0.005429x_3 + 0.053014x_4 + 0.010657x_5 + 0.01029x_6 + e 
where:
y = Predicted Salary
x_1 = Education Level
x_2 = Job Grade
x_3 = Year Hired
x_4 = Gender
x_5 = Performance Review
x_6 = Years of Experience
e = error term **

## Interpretation and Conclusion
When beginning to analyse out data, we had our initial hypothesis. We believed that the most important variables would include Age, EducLev, YrsExper, ITJob, JobGrade and Gender. These were based off the effects of society on certain factors especially for gender where there is a clear gender gap between male and female employees. It was also expected that education levels and years’ experience would positively affect salary. We thought however, that the YrHired wouldn’t have too much of an effect as well as for performance review which we assumed could be very biased. However, as seen Age and ITJob weren’t significant as they were eliminated during the stepwise process. While Age being removed did not seem too surprising as years’ experience has strong significance. It was interesting to note that Advanced IT skills had no significant effect on Salary as one would expect any additional skills would result in a higher Salary. 
We went further to test the final model on a few of the employees. Employee A, with an age of 61, education level of 5, job grade: 6, year hired: 1988, male, performance review: 5 and 29 years’ experience. The actual salary is £122,000, whilst the model predicted gives £116,351. This is out by roughly 4.6% out from the actual figure. Also looking at the last person’s data, the 17-year-old earning £59,000, education level of 2, job grade: 1, year hired: 2017, male, performance review:6 and 0 years’ experience; The predicted salary is £55,723. These deviations are however understandable as Age, ITJob and YrBorn were removed which would have accounted for the minor differences and our R-squared value isn’t perfectly 1. 

Based on our findings, we would like to recommend that the bank improves on their gender pay gap, as it is quite disappointing that gender is a significant variable that still determines salary. We would also recommend that the performance review method is improved to minimalize the risk of bias, since we saw the possibility of bias from the start and perhaps the bank reassess the value of IT skills within the sector.

## Suggestion for further work
There are many methods that can be implemented to improve the quality of this model. Firstly, the method of obtaining data. We were not told how this information was obtained, whether it was in person, via email or interview. This brings suspicion on the quality and reliability of data supplied by each employee. One problem we mentioned was having to remove one of the employee’s data as the education level was N/A, which reduced the precision. This could have been prevented by ensuring that the method applied is used effectively enough to minimalize the possibility of error. 
Secondly, how the categories were defined. An example is the salaries of the employees, how were the salaries calculated? Was the calculation method defined prior to requesting the salary (e.g. does the salary include bonuses), since this could have influenced our initial opinion on the 17-year-olds, £59000 salary. This can have a large impact on the outcome of this analysis, as things like predefinition can influence the overall outcome. Similarly, we weren’t told how the individuals were selected, if it truly was random, like we have assumed for the model’s purpose. This would also show whether the model is biased, however we cannot determine that. Other methods include analysing other characteristics such as race or demand for their positions and acquiring further information about the location as size of the bank.

# Understanding the dynamics of what characterized high earning individuals

This part of the report serves to understand the dynamics of individuals who are earning £75000 or above, which we will refer to as “high fliers” and whether such individuals have particular characteristics that differentiate them from their lower earning counterparts. Of the 207 observations excluding individual with employee number 92 there are a total of 74 individuals that fall into this category. That’s an average of roughly 36% of individuals within the bank.
Assessing each independent variable against Salary rather interesting characteristics seem to arise. This was done through a statistical test known as a t-test which is an analysis of two populations means through the use of statistical examination; This test is commonly used in testing the difference between the samples when the variances of two normal distributions are not known as is the case with our two samples of high fliers and non-highfliers.(Staff,2018)
Upon conduction of individual t-tests of Salary against each independent variable the following results arose (It should be noted that this test was not conducted on Year Hired as an average Year would not be of much relevance):

```{r, highearning}
### Compare high flyer employees with non high flyers
## Create dummy variable to indicate "high flyer"
bank1$highflyer <- 0
bank1[bank1$Salary > 75,"highflyer"] <- 1
## check differences between highflyers and non high flyers on ordinal variables
t.test(YrHired~highflyer,data=bank1)# significant p value highfliers have on average a higher education level 
t.test(JobGrade~highflyer,data=bank1)#significant p value, highfliers on averages have  significantly higher positions
t.test(YrsExper~highflyer,data=bank1)#significant p value, highfliers on averages have more experience mean 12 years
t.test(Age~highflyer,data=bank1)# not a significant p value can't consider a difference
t.test(PerfRev~highflyer,data=bank1)#not a significant p value
## check differences between highflyers and non high flyers on binary variables
tbl.gender1 <- table(gender=bank1$Gender,highflyer=bank1$highflyer)
tbl.gender1
summary(tbl.gender1)# significant Chi sqr. almost equal amounts of men and woman highfliers but as a proportion of total men and women aprx 53% of men in bank highfliers while only 24.5% of women in the bank are highfliers
tbl.it <- table(ITJob=bank1$ITJob,highflyer=bank1$highflyer)
tbl.it
summary(tbl.it) #p value not significant at 5% and there are more individuals with IT that arent highfliers than those who are

```
1)	Education Levels 
The mean level for high fliers is 4.38 while for non-high fliers the mean is 3.57. This indicates that high fliers have at least an undergraduate degree or higher with non-high fliers mostly achieving university diploma or equivalent/undergraduate study with no degree awarded or below.
2)	Job Grade
The mean level for high fliers is 4.24 which is significantly higher than the mean for non-high fliers of 1.87 this indicates that high fliers on average have job grades ranging in the highest categories while their non-higher flier counterparts on average have the more individuals in the lowest job grades. 
3)	Years’ Experience
The mean level for high fliers is 11.80 while non-higher fliers had a mean of only 7.15. Which as expected suggests that to earn a higher salary more years of experiences are required; on average nearly 12 years. 
4)	Year Born and Age
As with the model calculated year born and age are essentially the same variables they referred to as being perfectly correlated as such we will only consider Age. While this test was less significant than that of the other variables, it cannot be ignored. The mean age for high fliers is 38.66, approx. 39 while non-high fliers mean is 33.28, approx. 33 suggesting on average that high fliers are generally older; this is consistent with Years’ Experience as to achieve more years’ experience one would generally have to be older. 
5)	Performance Review
The mean for High fliers is 4.86 while non-high fliers have a mean of 4.15. While these values don’t differ too significantly is does suggest that highfliers have a slightly higher performance on average.

For the remaining factors due to them being binary variables a different Statistical test was conducted known as the Chi-Square test. Chi square test for testing goodness of fit is used to decide whether there is any difference between the observed value and the expected value. (En.Wikipedia.org,2018)
6)	Gender
	Non-High flier	High Flier
Female	103	36
Male	30	38
The dynamics of individuals are as follows:

While there are fairly equal numbers of male and female highfliers what is interesting to note is that 55.8% of total males and only 25.9% of total females are High fliers. This suggests that although the bank employs more females than males it is more likely that a male in the bank will be a high-flier than not while a female only has approx. a 1-in-4 chance of being a highflier suggesting that females are less likely to occupy High flier positions.
7)	 Advanced IT Skills (IT Job)
	Non-High flier	High Flier
No IT Skills	51	39
Advance IT Skills	82	35
The dynamics are as follows:

The table suggests that although there is a higher proportion of individuals with advanced IT skills, only around 30% of them are high fliers while 43.3% of individuals without IT skills are higher fliers. This suggests that IT skills don’t affect whether individuals earn over £75000; it seems to be a disadvantage as those without IT skills are more likely to be high fliers.

Thus, it can be said that there are unique characteristics that determine the level of earnings of high fliers within the bank. This includes having higher education levels and jobs grades, more years’ experience (older employees) and those that perform better on average. It can also be said that a higher proportion of males are high fliers despite there being more female employees in the bank and that having advanced IT skills doesn’t correlate to greater chances of earning more. 


